{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 000 Forecasting Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from https://colab.research.google.com/drive/1_Il5h2Ed4zFa6Z3bROVCE68LZcSi4wHX?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from omegaconf import OmegaConf\n",
    "import datetime,json, os, requests, re\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-08-06'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "today = str(datetime.datetime.now())[0:10]\n",
    "today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run this notebook as is, you'll need to enter a few API keys (use the key icon on the left to input them):\n",
    "\n",
    "- `METACULUS_TOKEN`: you can find your Metaculus token under your bot's user settings page: https://www.metaculus.com/accounts/settings/, or on the bot registration page where you created the account: https://www.metaculus.com/aib/\n",
    "- `OPENAPI_API_KEY`: get one from OpenAIs page: https://platform.openai.com/settings/profile?tab=api-keys\n",
    "- `PERPLEXITY_API_KEY` - used to search up-to-date information about the question. Get one from https://www.perplexity.ai/settings/api\n",
    "- `ASKNEWS_CLIENT_ID`, `ASKNEWS_SECRET`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_fn = \"tokens.yaml\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tokens = OmegaConf.create(\"\"\"\n",
    "METACULUS_TOKEN: xx\n",
    "OPENAI_API_KEY: yy\n",
    "OPENAI_MODEL: gpt-4o\n",
    "PERPLEXITY_API_KEY: zz\n",
    "PERPLEXITY_MODEL: llama-3-sonar-large-32k-online\"\"\")\n",
    "OmegaConf.save(config=tokens, f=token_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = OmegaConf.load(token_fn)\n",
    "\n",
    "def pr(tokens):\n",
    "    print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDukuXArbgdm"
   },
   "source": [
    "## LLM and Metaculus Interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDukuXArbgdm"
   },
   "source": [
    "This section sets up some simple helper code you can use to get data about forecasting questions and to submit a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HifodCwcGU0j"
   },
   "outputs": [],
   "source": [
    "AUTH_HEADERS = {\"headers\": {\"Authorization\": f\"Token {config.METACULUS_TOKEN}\"}}\n",
    "API_BASE_URL = \"https://www.metaculus.com/api2\"\n",
    "WARMUP_TOURNAMENT_ID = 3349\n",
    "SUBMIT_PREDICTION = True\n",
    "\n",
    "def find_number_before_percent(s):\n",
    "    # Use a regular expression to find all numbers followed by a '%'\n",
    "    matches = re.findall(r'(\\d+)%', s)\n",
    "    if matches:\n",
    "        # Return the last number found before a '%'\n",
    "        return int(matches[-1])\n",
    "    else:\n",
    "        # Return None if no number found\n",
    "        return None\n",
    "\n",
    "def post_question_comment(question_id, comment_text):\n",
    "    \"\"\"\n",
    "    Post a comment on the question page as the bot user.\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.post(\n",
    "        f\"{API_BASE_URL}/comments/\",\n",
    "        json={\n",
    "            \"comment_text\": comment_text,\n",
    "            \"submit_type\": \"N\",\n",
    "            \"include_latest_prediction\": True,\n",
    "            \"question\": question_id,\n",
    "        },\n",
    "        **AUTH_HEADERS,\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    print(\"Comment posted for \", question_id)\n",
    "\n",
    "def post_question_prediction(question_id, prediction_percentage):\n",
    "    \"\"\"\n",
    "    Post a prediction value (between 1 and 100) on the question.\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE_URL}/questions/{question_id}/predict/\"\n",
    "    response = requests.post(\n",
    "        url,\n",
    "        json={\"prediction\": float(prediction_percentage) / 100},\n",
    "        **AUTH_HEADERS,\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    print(\"Prediction posted for \", question_id)\n",
    "\n",
    "\n",
    "def get_question_details(question_id):\n",
    "    \"\"\"\n",
    "    Get all details about a specific question.\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE_URL}/questions/{question_id}/\"\n",
    "    response = requests.get(\n",
    "        url,\n",
    "        **AUTH_HEADERS,\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return json.loads(response.content)\n",
    "\n",
    "def list_questions(tournament_id=WARMUP_TOURNAMENT_ID, offset=0, count=1000):\n",
    "    \"\"\"\n",
    "    List (all details) {count} questions from the {tournament_id}\n",
    "    \"\"\"\n",
    "    url_qparams = {\n",
    "        \"limit\": count,\n",
    "        \"offset\": offset,\n",
    "        \"has_group\": \"false\",\n",
    "        \"order_by\": \"-activity\",\n",
    "        \"forecast_type\": \"binary\",\n",
    "        \"project\": tournament_id,\n",
    "        \"status\": \"open\",\n",
    "        \"type\": \"forecast\",\n",
    "        \"include_description\": \"true\",\n",
    "    }\n",
    "    url = f\"{API_BASE_URL}/questions/\"\n",
    "    response = requests.get(url, **AUTH_HEADERS, params=url_qparams)\n",
    "    response.raise_for_status()\n",
    "    data = json.loads(response.content)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IFP:\n",
    "\n",
    "    forecast_fields = ['question_id',\n",
    "                       'title',\n",
    "                       'feedback',\n",
    "                       'resolution_criteria', \n",
    "                       'background', \n",
    "                       'event', \n",
    "                       'model_domain']\n",
    "\n",
    "    forecast_format = f\"|{'|'.join(forecast_fields)}|\"\n",
    "\n",
    "    openai_max_tokens = 30000\n",
    "\n",
    "    def format(self):\n",
    "        rec = vars(self)\n",
    "        fmt = '|'.join([f\"{x}: {rec[x]}\" for x in self.forecast_fields])\n",
    "        return '|' + fmt + '|'\n",
    "\n",
    "    def over_openai_max(self):\n",
    "        sep = self.format()\n",
    "        return len(sep.split(' ')) > self.openai_max_tokens # Max limit for OpenAI\n",
    "\n",
    "    def record(self):\n",
    "        return self.format()\n",
    "        if self.over_openai_max(): \n",
    "            self.news = ' '.join(self.news.split(' ')[0:int(0.5*self.openai_max_tokens)])\n",
    "        if self.over_openai_max(): \n",
    "            self.research = ' '.join(self.research.split(' ')[0:int(0.5*self.openai_max_tokens)])\n",
    "        if self.over_openai_max(): \n",
    "            raise Exception('IFP record over OpenAI max')\n",
    "        return self.format()\n",
    "\n",
    "    def __init__(self, question_id):\n",
    "        self.question_id = question_id\n",
    "        self.question_details = get_question_details(self.question_id)\n",
    "        self.today = datetime.datetime.now().strftime(\"%Y-%m-%d\")   \n",
    "        self.title = self.question_details[\"title\"]\n",
    "        self.resolution_criteria = self.question_details[\"resolution_criteria\"]\n",
    "        self.background = self.question_details[\"description\"]\n",
    "        self.fine_print = self.question_details[\"fine_print\"]\n",
    "        self.event = ''\n",
    "        self.model_domain = ''\n",
    "        self.feedback = ''\n",
    "\n",
    "    def report(self):\n",
    "        rpt = f\"\"\"\n",
    "The future event is described by this question: [ {self.title} ]\n",
    "The resolution criteria are: [ {self.resolution_criteria} ]\n",
    "The background is: [ {self.background} ]\"\"\"\n",
    "        if self.fine_print:\n",
    "            rpt += f\"\"\"\n",
    "The fine print is: [ {self.fine_print} ]\"\"\"\n",
    "        return rpt\n",
    "\n",
    "    def upload(self):\n",
    "        post_question_prediction(self.question_id, self.forecast)\n",
    "        post_question_comment(self.question_id, self.rationale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_test = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if unit_test:\n",
    "    qid = 26775\n",
    "    ifp = IFP(qid)\n",
    "    ifps = {qid: ifp}\n",
    "    print(ifp.record())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News source (AskNews)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pip install asknews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1tc383HraMZOiyfKFF1EXAtlTYbsuv3Q5?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from asknews_sdk import AskNewsSDK\n",
    "\n",
    "def asknews_to_dict(x):\n",
    "    return dict([x.split(': ', 1) for x in x.split('\\n')])\n",
    "\n",
    "class AskNews():\n",
    "\n",
    "    def __init__(self):\n",
    "        ASKNEWS_CLIENT_ID = config.ASKNEWS_CLIENT_ID\n",
    "        ASKNEWS_SECRET = config.ASKNEWS_SECRET\n",
    "        \n",
    "        self.ask = AskNewsSDK(\n",
    "              client_id=config.ASKNEWS_CLIENT_ID,\n",
    "              client_secret=config.ASKNEWS_SECRET,\n",
    "              scopes=[\"news\"]\n",
    "          )\n",
    "\n",
    "        self.cache = {}\n",
    "        \n",
    "    def query(self, \n",
    "              q, \n",
    "              method,    # use \"nl\" for natural language for your search, or \"kw\" for keyword, or 'both'\n",
    "              strategy): # strategy=\"latest news\" enforces looking at the latest news only\n",
    "                         # strategy=\"news knowledge\" looks for relevant news within the past 60 days\n",
    "        return self.ask.news.search_news(\n",
    "            query=q, # your keyword query\n",
    "            n_articles=10, # control the number of articles to include in the context\n",
    "            return_type=\"dicts\",  # you can also ask for \"dicts\" if you want more information\n",
    "            method=method, \n",
    "            strategy=strategy).as_dicts # strategy=\"latest news\" enforces looking at the latest news only,\n",
    "\n",
    "    def multi_strategy(self, q):\n",
    "        all = []\n",
    "        for strategy in ['latest news', 'news knowledge']:\n",
    "            for x in self.query(q, 'both', strategy):\n",
    "                all.append(x.summary)\n",
    "        all = list(set(all))\n",
    "        return '\\n\\n'.join(all)\n",
    "\n",
    "    def research(self, event, group):\n",
    "        q = '\\n'.join(ifp.title.strip() for ifp in group)\n",
    "        try:\n",
    "            return self.cache[q]\n",
    "        except:\n",
    "            news = self.multi_strategy(q)\n",
    "            self.cache[q] = news\n",
    "        return news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if unit_test:\n",
    "    ask = AskNews()\n",
    "    event = '2024 Grand Chess Tour'\n",
    "    group = [IFP(id) for id in [26771, 26772, 26773, 26774]]\n",
    "    news = ask.research(event, group)\n",
    "    print(news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM:\n",
    "    def __init__(self, system_role):\n",
    "        self.system_role = system_role\n",
    "        self.messages = [{\"role\": \"system\", \"content\": system_role}]\n",
    "        self.cache = {}\n",
    "\n",
    "    def chat(self, query):\n",
    "        try:\n",
    "            return self.cache[query]\n",
    "        except:\n",
    "            self.messages.append({\"role\": \"user\", \"content\": query})\n",
    "            text = self.message()\n",
    "            self.messages.append({\"role\": \"assistant\", \"content\": text})\n",
    "            self.cache[query] = text\n",
    "            return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MetaAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pypi.org/project/meta-ai-api/1.0.6/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meta_ai_api import MetaAI as mai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaAI(LLM):\n",
    "\n",
    "    def __init__(self, system_role = \"Nice talker\"):\n",
    "        self.ai = mai()\n",
    "        super().__init__(system_role)\n",
    "        \n",
    "    def message(self):\n",
    "        self.response = self.ai.prompt(message=self.messages[-1]['content'])\n",
    "        return self.response['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if unit_test:\n",
    "    ai = MetaAI()\n",
    "    r = ai.chat(ifp.title)\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingChat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pypi.org/project/hugchat/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hugchat import hugchat\n",
    "from hugchat.login import Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingChat(LLM):\n",
    "\n",
    "    def __init__(self, system_role = \"Nice talker\"):\n",
    "        # Log in to huggingface and grant authorization to huggingchat\n",
    "        EMAIL = config.HUGGINGFACE_USERNAME\n",
    "        PASSWD = config.HUGGINGFACE_PASSWORD\n",
    "        cookie_path_dir = \"./cookies/\" # NOTE: trailing slash (/) is required to avoid errors\n",
    "        sign = Login(EMAIL, PASSWD)\n",
    "        cookies = sign.login(cookie_dir_path=cookie_path_dir, save_cookies=True)\n",
    "        self.chatbot = hugchat.ChatBot(cookies=cookies.get_dict()) \n",
    "        super().__init__(system_role)\n",
    "        \n",
    "    def message(self):\n",
    "        result = ''.join([x['token'] for x in self.chatbot.chat(self.system_role + '\\n\\n' + self.messages[-1]['content']) if x])\n",
    "        return result\n",
    "\n",
    "    def web_search(self, query):\n",
    "        query_result = self.chatbot.query(query, web_search=True)\n",
    "        return query_result\n",
    "        print(query_result)\n",
    "        for source in query_result.web_search_sources:\n",
    "            print(source.link)\n",
    "            print(source.title)\n",
    "            print(source.hostname)\n",
    "\n",
    "    def available_models(self):\n",
    "        return [(i,str(x)) for i,x in enumerate(self.chatbot.get_available_llm_models())]\n",
    "\n",
    "    def switch_llm(self, i):\n",
    "        self.chatbot.switch_llm(i)\n",
    "\n",
    "    def conversation_info(self):\n",
    "        info = self.chatbot.get_conversation_info()\n",
    "        # print(info.id, info.title, info.model, info.system_prompt, info.history)\n",
    "        return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if unit_test:\n",
    "    hc = HuggingChat()\n",
    "    hc.switch_llm(1)\n",
    "    print(hc.chat(ifp.title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metaculus Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class Claude(LLM):\n",
    "       \n",
    "    def message(self):\n",
    "        content = self.messages[-1]['content']\n",
    "        url = \"https://www.metaculus.com/proxy/anthropic/v1/messages\"\n",
    "        headers =  {\"Authorization\": f\"Token {config.METACULUS_TOKEN}\",\n",
    "                    \"anthropic-version\": \"2023-06-01\",\n",
    "                    \"content-type\": \"application/json\"}\n",
    "        data = {\"model\": \"claude-3-5-sonnet-20240620\",\n",
    "                \"max_tokens\": 1024,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": self.system_role + '\\n\\n' + content}]}\n",
    "        response = requests.post(url, headers=headers, data = json.dumps(data))\n",
    "        # Check the response status code\n",
    "        if response.status_code != 200:\n",
    "            print(\"Request failed with status code\", response.status_code)\n",
    "            print(response.text)\n",
    "        return response.json()['content'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if unit_test:\n",
    "    claude = Claude('Please help me.')\n",
    "    print(claude.chat(ifp.title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perplexity(LLM):\n",
    "    def message(self):\n",
    "        url = \"https://api.perplexity.ai/chat/completions\"\n",
    "        headers = {\n",
    "            \"accept\": \"application/json\",\n",
    "            \"authorization\": f\"Bearer {config.PERPLEXITY_API_KEY}\",\n",
    "            \"content-type\": \"application/json\"  }\n",
    "        payload = {\"model\": config.PERPLEXITY_MODEL, \"messages\": self.messages }\n",
    "        response = requests.post(url=url, json=payload, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if unit_test:\n",
    "    perp = Perplexity('You are the most deeply knowledgeable web search engine and insightful analyst')\n",
    "    print(perp.chat(ifp.title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metaculus OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class MetacGPT(LLM):\n",
    "\n",
    "    def __init__(self, system_role):\n",
    "        super().__init__(system_role)\n",
    "        \n",
    "    def message(self):\n",
    "        url = \"https://www.metaculus.com/proxy/openai/v1/chat/completions\"\n",
    "        headers =  {\"Authorization\": f\"Token {config.METACULUS_TOKEN}\",\n",
    "                    \"content-type\": \"application/json\"}\n",
    "        data = {\"model\": \"gpt-4o\",\n",
    "                \"max_tokens\": 1024,\n",
    "                \"messages\": self.messages}\n",
    "        response = requests.post(url, headers=headers, data = json.dumps(data))\n",
    "        # Check the response status code\n",
    "        if response.status_code != 200:\n",
    "            print(\"Request failed with status code\", response.status_code)\n",
    "            print(response.text)\n",
    "        self.rec = response.json()\n",
    "        return self.rec ['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if unit_test:\n",
    "    mgpt = MetacGPT('Please help me')\n",
    "    print(mgpt.chat('Bruce Bueno de Mesquita'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatGPT(LLM):\n",
    "    def __init__(self, system_role):\n",
    "        super().__init__(system_role)\n",
    "        self.client = OpenAI(api_key=config.OPENAI_API_KEY)\n",
    "\n",
    "    def message(self):\n",
    "        chat_completion = self.client.chat.completions.create(\n",
    "            model=config.OPENAI_MODEL,\n",
    "            messages= self.messages)\n",
    "        return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if unit_test:\n",
    "    oai = ChatGPT('You are the most deeply knowledgeable web search engine and insightful analyst')\n",
    "    print(oai.chat(ifp.title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WUvm1tVmMkO"
   },
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, system_role, llm):\n",
    "        self.llm = llm(system_role)\n",
    "\n",
    "    def chat(self, prompt):\n",
    "        return self.llm.chat(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question correlator for common topic inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionRelator(Agent):\n",
    "    def __init__(self, llm):\n",
    "        self.system_role = f\"\"\"\n",
    "You are prompted with list of forecasting questions, each with an id and a title.\n",
    "Label each question with an underlying event.\n",
    "If the questions are for the same event, use the same name for the underlying event.\n",
    "Please return as separate lines formatted as |event|id|title|, do not add any other formatting.\n",
    "\"\"\"\n",
    "        super().__init__(self.system_role, llm)\n",
    "\n",
    "    def relate(self, ifps):\n",
    "        prompt = '\\n'.join([f\"{ifp.question_id}: {ifp.title}\" for ifp in ifps.values()])\n",
    "        KL = self.chat(prompt)\n",
    "        print(KL)\n",
    "        K1 = [x.split('|') for x in KL.split('\\n')]\n",
    "        self.topic_map = [(int(id),event) for _,event,id,_,_ in K1]\n",
    "        self.topics = set([event for id,event in self.topic_map])\n",
    "        self.topic_groups = {topic: [ifps[x] for x,y in self.topic_map if y == topic] for topic in self.topics}\n",
    "        for id,event in self.topic_map :\n",
    "            ifps[id].event = event\n",
    "        return self.topic_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if unit_test:\n",
    "    try:\n",
    "        ifps\n",
    "    except:\n",
    "        test_day = '01AUG24'\n",
    "        ifps = {id: IFP(id) for id in history[test_day]}\n",
    "    qr = QuestionRelator(ChatGPT)\n",
    "    print(qr.relate(ifps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model domain classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelDomainClassifier(Agent):\n",
    "    def __init__(self, llm):\n",
    "        self.system_role = f\"\"\"\n",
    "A question about an event is formatted as |id|question|criteria|background|fineprint|.\n",
    "You will report the model domain of the question, one of\n",
    "\n",
    "Rate over time\n",
    "Election outcome\n",
    "Sports performance\n",
    "Military conflict\n",
    "Civil unrest\n",
    "Dated product announcement\n",
    "Market price\n",
    "Macroeconomics\n",
    "Epidemic\n",
    "Disease\n",
    "Drug discovery\n",
    "Medical device\n",
    "Crime\n",
    "Leadership change\n",
    "Other\n",
    "\"\"\"\n",
    "        super().__init__(self.system_role, llm)\n",
    "\n",
    "    def classify(self, ifp):\n",
    "        prompt = f\"|{ifp.question_id}|{ifp.title}|{ifp.resolution_criteria}|{ifp.background}|{ifp.fine_print}|\"\n",
    "        self.R = self.chat(prompt)\n",
    "        ifp.model_domain = self.R\n",
    "        print(ifp.question_id, ifp.title)\n",
    "        print(self.R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if unit_test:\n",
    "    mdc = ModelDomainClassifier(ChatGPT)\n",
    "    mdc.classify(ifp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rate Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RateAnalyzer(Agent):\n",
    "    def __init__(self, llm):\n",
    "        self.system_role = f\"\"\"\n",
    "A group of events are given, formatted as \"GROUP\" followed by a group of questions separated by carriage returns, followed by \"NEWS\" followed by news on the topic.\n",
    "Today's date is {today}.\n",
    "If the question is about a level that changes at some rate over time, you will report \n",
    "1. today's date, \n",
    "2. the end date of the question, \n",
    "3. the time in days D from today to end date, \n",
    "4. the daily rate of change R of the quantity,\n",
    "5. today's value V of the quantity,\n",
    "6. the change in value dV of the quantity = D * r,\n",
    "7. the final value of the quantity F = V + dV.  Give your best estimate.\n",
    "Otherwise if the question is about the date that an event will occur, you will report\n",
    "1. today's date, \n",
    "2. the end (measurement) date of the question, \n",
    "3. the time in days D from today to end date, \n",
    "4. the specific date at which the event is most likely to occur, without reference to the end date of the question\n",
    "5. whether the likely event date is before, or on or after the question end date\n",
    "\n",
    "Provide only one type of answer, not both.\n",
    "\"\"\"\n",
    "        super().__init__(self.system_role, llm)\n",
    "\n",
    "    def assess(self, group, news):\n",
    "        titles = '\\n'.join([ifp.title.strip() for ifp in group])\n",
    "        self.q = f'GROUP\\n{titles}\\nNEWS\\n{news}'\n",
    "        self.rates = self.chat(self.q)\n",
    "        return self.rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if unit_test:\n",
    "    try:\n",
    "        group\n",
    "    except:\n",
    "        group = [IFP(id) for id in [26771, 26772, 26773, 26774]]\n",
    "\n",
    "    rates = RateAnalyzer(ChatGPT)\n",
    "    r = rates.assess(group, news)\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Superforecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Superforecaster(Agent):\n",
    "    def __init__(self, llm):\n",
    "        self.system_role = f\"\"\"\n",
    "You are a superforecaster.  \n",
    "You assign a probability to questions about events.\n",
    "Questions are given as GROUP|group_description followed by NEWS|group_news followed by RATES|group_rates \n",
    "followed by the questions in the group formatted QUESTIONS followed by, for each question, {IFP.forecast_format}.\n",
    "Questions which are about the same event should be assigned consistent probabilities.\n",
    "Reply to questions with |ASSESSMENT|id|ZZ|rationale| where ZZ is an integer probability from 1 to 99 and rationale is your reasoning for the forecast.\n",
    "Separate each question with '^^^'.\n",
    "Do not add any additional headings or group labels or other formatting.\n",
    "After your initial forecast you may receive feedback of form |CRITIC|id|feedback|.\n",
    "Reply to each feedback with |id|ZZ|rationale| where ZZ is an integer probability from 1 to 99 and \n",
    "and rationale is a revised assessment which may be adjusted from a prior assessment due the feedback unless the feedback is \"I concur\".\n",
    "\"\"\"\n",
    "        super().__init__(self.system_role, llm)\n",
    "\n",
    "    def forecast(self, event, news, rates, group):\n",
    "        ifps = '\\n'.join([ifp.record() for ifp in group])\n",
    "        prompt = f\"GROUP|{event}\\nNEWS|{news}\\nRATES|{rates}\\nQUESTIONS\\n{ifps}\"\n",
    "        self.F0 = self.chat(prompt)\n",
    "        self.F1 = [x.strip().replace('\\n', '') for x in self.F0.split('^^^')]\n",
    "        self.F2 = [x.split('|') for x in self.F1] \n",
    "        self.F3 = [[x for x in y if x] for y in self.F2]\n",
    "        self.F4 = [(int(id),int(forecast),rationale) for _, id, forecast, rationale in self.F3]\n",
    "        self.ifps = {ifp.question_id: ifp for ifp in group}\n",
    "        for id, forecast, rationale in self.F4:\n",
    "            self.ifps[id].forecast = forecast\n",
    "            self.ifps[id].rationale = rationale\n",
    "            print(id, forecast, rationale)\n",
    "\n",
    "    def reassess(self, ifp):\n",
    "        prompt = f\"|CRITIC|{ifp.question_id}|{ifp.feedback}|\"\n",
    "        self.R0 = self.chat(prompt)\n",
    "        id,fcst,rationale = [x for x in self.R0.strip().split('|') if x]\n",
    "        id = int(id)\n",
    "        fcst = int(fcst)\n",
    "        ifps[id].forecast = fcst\n",
    "        ifps[id].rationale = rationale\n",
    "        print(id, fcst, rationale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if unit_test:\n",
    "    sf = Superforecaster(ChatGPT)\n",
    "    event = '2024 Grand Chess Tour'\n",
    "    group = qr.topic_groups[event]\n",
    "    sf.forecast(event, news, r, group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic (Agent):\n",
    "    def __init__(self, llm):\n",
    "\n",
    "        self.system_role = f\"\"\"\n",
    "You a very smart and worldly person reviewing a superforecaster's assignment of probabilities to events.\n",
    "You will receive an event with probabilities given as |event|id|question|zz|rationale|.\n",
    "zz is an integer probability from 1 to 99 and rationale is the student's logic for assigning probability of zz.\n",
    "You will reply with a line |id|feedback| where feedback is \"I concur\" if you see no problem with the rationale and zz otherwise presents possible problems with the rationale and zz.\n",
    "\"\"\"\n",
    "        super().__init__(self.system_role, llm)\n",
    "\n",
    "    def feedback(self, ifp):\n",
    "        self.fb = 'I prasldfkbeddy.'\n",
    "        prompt = f\"|{ifp.event}|{ifp.question_id}|{ifp.title}|{ifp.forecast}|{ifp.rationale}|\"\n",
    "        self.fb = self.chat(prompt)\n",
    "        if '|' in self.fb: # LLM obeyed system role\n",
    "            self.fb1 = self.fb.split('|')[2:]\n",
    "            self.fb = ''.join([x.strip() for x in self.fb1 if x])\n",
    "        ifp.feedback = self.fb \n",
    "        print(ifp.question_id, ifp.feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if unit_test:\n",
    "    for llm in [MetaAI, Claude, Perplexity, ChatGPT, HuggingChat]:\n",
    "        print(llm.__name__)\n",
    "        critic = Critic(llm)\n",
    "        critic.feedback(ifp) \n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add an agent to summarize the back and forth between critic and forecaster into a single cogent rationale that incorporates all that was discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IFP history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = { '08JUL24': [25876, 25877, 25875, 25873, 25871, 25878, 25874, 25872],\n",
    "            '09JUL24': [26006, 25936, 25935, 25934, 25933, 26004, 26005],\n",
    "            '10JUL24': [25955, 25956, 25957, 25960, 25959, 25954, 25953, 25952, 25958],\n",
    "            '11JUL24': [26019, 26018, 26017, 26020, 26022, 26021, 26023, 26024],\n",
    "            '12JUL24': [26095, 26096, 26097, 26098, 26099, 26100, 26101, 26102],\n",
    "            '15JUL24': [26133, 26134, 26138, 26139, 26140, 26157, 26158, 26159],\n",
    "            '16JUL24': [26189, 26190, 26191, 26192, 26193, 26194, 26195, 26196],\n",
    "            '17JUL24': [26210, 26211, 26212, 26213, 26214, 26215, 26216],\n",
    "            '18JUL24': [26232, 26233, 26234, 26235, 26236],\n",
    "            '19JUL24': [26302, 26303, 26304, 26305, 26306, 26307],\n",
    "            '22JUL24': [26387, 26388, 26389, 26390, 26391, 26392],\n",
    "            '23JUL24': [26404, 26405, 26406, 26407, 26408],\n",
    "            '24JUL24': [26550, 26551, 26552, 26553, 26554, 26555],\n",
    "            '25JUL24': [26568, 26569, 26570, 26571, 26572, 26573, 26574, 26575, 26576, 26577],\n",
    "            '26JUL24': [26638, 26639, 26640, 26641, 26642, 26643, 26644, 26645, 26646],\n",
    "            '29JUL24': [26665, 26666, 26667, 26668, 26669, 26670, 26671, 26683],\n",
    "            '30JUL24': [26700, 26701, 26702, 26703, 26704, 26705, 26706],\n",
    "            '31JUL24': [26816, 26817, 26818, 26819, 26820, 26821, 26844],\n",
    "            '01AUG24': [26771, 26772, 26773, 26774, 26775, 26776, 26777, 26778, 26779, 26780, 26781],\n",
    "            '02AUG24': [26837, 26838, 26839, 26840, 26841, 26842],\n",
    "            '05AUG24': [26913, 26914, 26915, 26916, 26917, 26918, 26919, 26920],\n",
    "            '06AUG24': [26957, 27019, 27020, 27021, 27022]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-06 [26957, 27019, 27020, 27021, 27022]\n"
     ]
    }
   ],
   "source": [
    "ifps = list_questions()['results']\n",
    "today_ids = list(sorted([x['id'] for x in ifps]))\n",
    "print(today, today_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifps = {id: IFP(id) for id in today_ids}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Forecaster:\n",
    "    \n",
    "    def __init__(self, ifps):\n",
    "        self.max_tries = 4\n",
    "        self.qr = QuestionRelator(ChatGPT)\n",
    "\n",
    "        self.ask = AskNews()\n",
    "        self.critic = Critic(HuggingChat)\n",
    "        self.mdc = ModelDomainClassifier(ChatGPT)\n",
    "        self.ifps = ifps\n",
    "        \n",
    "    def fit(self):\n",
    "        print('MODEL DOMAIN')\n",
    "        for ifp in self.ifps.values():\n",
    "            self.mdc.classify(ifp)\n",
    "        print()  \n",
    "        print('CORRELATOR')\n",
    "        self.rho = self.qr.relate(self.ifps)\n",
    "        for event, group in self.rho.items():\n",
    "            news = self.ask.research(event, group)    \n",
    "            ra = RateAnalyzer(ChatGPT)\n",
    "            rates = ra.assess(group, news)\n",
    "            sf = Superforecaster(ChatGPT)\n",
    "            sf.forecast(event, news, rates, group)\n",
    "            critic = Critic(HuggingChat)\n",
    "            for ifp in group:\n",
    "                print(\"Refining\", ifp.question_id)\n",
    "                for i in range(self.max_tries):\n",
    "                    print(\"Pass\", i, \"of\", self.max_tries, \"on\", ifp.question_id)\n",
    "                    if 'concur' in ifp.feedback:\n",
    "                        print(\"concur 1\")\n",
    "                        break\n",
    "                    critic.feedback(ifp)\n",
    "                    if 'concur' in ifp.feedback:\n",
    "                        print(\"concur 2\")\n",
    "                        break\n",
    "                    sf.reassess(ifp)\n",
    "                print(\"===============================================\")\n",
    "\n",
    "    def report(self):\n",
    "        for ifp in self.ifps.values():\n",
    "            print(ifp.question_id, ifp.title)\n",
    "            print(\"Forecast\", ifp.forecast)\n",
    "            print(\"Rationale\", ifp.rationale, '\\n')\n",
    "\n",
    "    def upload(self):\n",
    "        for ifp in self.ifps.values():\n",
    "            ifp.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if unit_test:\n",
    "    test_day = '01AUG24'\n",
    "    ifps = {id: IFP(id) for id in history[test_day]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = Forecaster(ifps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL DOMAIN\n",
      "26957 Will Iran carry out a deadly attack within Israel before September 1, 2024?\n",
      "**Model Domain: Military conflict**\n",
      "27019 Will there be a debate between Kamala Harris and Donald Trump on September 4, 2024?\n",
      "**Model Domain: Leadership change**\n",
      "27020 Before October 1, 2024, will Anthropic announce on the news section of its website that it is planning an IPO?\n",
      "**Model Domain: Dated product announcement**\n",
      "27021 Will the USA win more Gold than Silver medals at the Paris 2024 Olympics?\n",
      "**Model Domain: Sports performance**\n",
      "27022 Will astronauts Suni Williams and Butch Wilmore be on Earth on September 15, 2024?\n",
      "**Model Domain: Other**\n",
      "\n",
      "CORRELATOR\n",
      "|Iran-Israel military conflict|26957|Will Iran carry out a deadly attack within Israel before September 1, 2024?|\n",
      "|US Presidential election|27019|Will there be a debate between Kamala Harris and Donald Trump on September 4, 2024?|\n",
      "|Anthropic IPO|27020|Before October 1, 2024, will Anthropic announce on the news section of its website that it is planning an IPO?|\n",
      "|Paris 2024 Olympics|27021|Will the USA win more Gold than Silver medals at the Paris 2024 Olympics?|\n",
      "|Astronauts' location|27022|Will astronauts Suni Williams and Butch Wilmore be on Earth on September 15, 2024?|\n",
      "27019 65 Given that Trump has formally announced his willingness to debate Harris on September 4 and has even publicized the event details, there is a fair chance that the debate will occur. However, the Harris campaign’s firm stance on the September 10 date and the absence of a confirmed agreement from both parties introduces significant uncertainty. Thus, I assign a probability of 65%, reflecting the possibility that negotiations could still sway the outcome towards the debate happening on the proposed date.\n",
      "Refining 27019\n",
      "Pass 0 of 4 on 27019\n",
      "27019 Your rationale seems mostly sound, but I'd caution against overestimating the influence of Trump's public announcement. Given the history of presidential debate negotiations, the absence of a confirmed agreement from both parties should weigh more heavily in your assessment. Additionally, the Harris campaign's firm stance on the September 10 date suggests a strong preference that might not be easily swayed. I'd consider revising your probability downward to reflect these factors, potentially to the 40-50% range.\n",
      "27019 50 I appreciate the feedback. The lack of a confirmed agreement from both parties and the strong stance of the Harris campaign indeed weigh heavily on the probability. Considering these factors alongside the historical context of debate negotiations, I revise my probability to 50%, reflecting the equal uncertainty of this event occurring as announced by Trump.\n",
      "Pass 1 of 4 on 27019\n",
      "27019 I concur\n",
      "concur 2\n",
      "===============================================\n",
      "27020 20 Given the current environment and the absence of concrete indicators suggesting an imminent IPO, it is unlikely that Anthropic will announce an IPO before October 1, 2024. The consistent funding streams from significant investors like Google and Amazon provide the company with ample resources, potentially delaying any urgency for going public. Additionally, the economic news surrounding major tech companies and the expressed caution in the market do not favor an IPO announcement in such a short timeframe.\n",
      "Refining 27020\n",
      "Pass 0 of 4 on 27020\n",
      "27020 I concur\n",
      "concur 2\n",
      "===============================================\n",
      "27022 70 Given the latest information, it is clear that there are significant technical issues with the Boeing Starliner capsule that have delayed the astronauts' return. There is a strong possibility that NASA may opt to use a SpaceX Dragon capsule for the rescue, as the Starliner's battery life only extends until early September. While NASA and Boeing express confidence in the Starliner, the unresolved thruster issues and helium leaks make the situation uncertain. Given the contingency planning and urgency portrayed, there is a 70% chance that the astronauts will be back on Earth by September 15, 2024.\n",
      "Refining 27022\n",
      "Pass 0 of 4 on 27022\n",
      "27022 Your rationale is thorough and well-reasoned, but I'm concerned that the assigned probability of 70% might be overly optimistic given the significant technical issues with the Boeing Starliner capsule. While it's true that NASA may opt for a SpaceX Dragon capsule, there are still many uncertainties surrounding the rescue mission, including the thruster issues and helium leaks. Furthermore, the battery life of the Starliner only extending until early September adds an extra layer of complexity. Considering these factors, I would have expected a slightly lower probability, perhaps in the 40-60% range. I concur with your analysis, but would like to see a more detailed explanation for why you settled on 70%.\n",
      "concur 2\n",
      "===============================================\n",
      "26957 85 The current intelligence and news reports indicate a high likelihood of a significant attack by Iran against Israel in retaliation for the assassinations of Ismail Haniyeh and Fuad Shukr. The window for this attack appears to be within the next 72 hours, with both Iranian and US officials acknowledging the imminent threat. Considering historical precedents where Iran has launched significant retaliatory attacks after provocations and the amassed military movements, an attack resulting in casualties within Israel before September 1, 2024, seems very probable.\n",
      "Refining 26957\n",
      "Pass 0 of 4 on 26957\n",
      "26957 While your rationale highlights relevant historical precedents and current tensions, the assigned probability of 85 seems overly confident given the complexities of geopolitical decision-making and the inherent uncertainty in predicting specific military actions. It might be more prudent to consider a slightly lower probability, acknowledging the possibility of diplomatic efforts or internal Iranian decision-making processes that could delay or prevent an attack. Additionally, the rationale heavily relies on short-term intelligence and news reports, which can be subject to change or revision. A more nuanced approach might incorporate a broader range of scenarios and timeframes.\n",
      "26957 75 I acknowledge the feedback regarding the complexity and potential for diplomatic interventions or changes in internal Iranian decision-making affecting the likelihood of an attack. Adjusting the probability to account for these variables provides a more nuanced assessment. While recent intelligence and military movements suggest a high likelihood of a retaliatory strike, it is prudent to temper confidence considering the dynamic nature of geopolitical events. A probability of 75% reflects a high likelihood while accommodating the inherent uncertainties.\n",
      "Pass 1 of 4 on 26957\n",
      "26957 I concur\n",
      "concur 2\n",
      "===============================================\n",
      "27021 60 The United States currently leads the medal count and has historically performed well in the final days of the Olympics, especially in track and field and team sports like basketball that award multiple gold medals. While the US currently trails China in gold medals, they have many strong contenders in upcoming events which can lead to a surge in gold medals. Given this information, it is moderately likely that the USA will win more golds than silvers by the end of the Games.\n",
      "Refining 27021\n",
      "Pass 0 of 4 on 27021\n",
      "27021 Your rationale is mostly sound, but you may be overestimating the USA's chances of winning more golds than silvers. While the US has historically performed well in certain events, the Olympics are inherently unpredictable, and upsets can occur. Additionally, you mention that the US currently trails China in gold medals, which suggests that the task of surpassing China's gold count is significant. Furthermore, your probability of 60 implies a roughly 3:2 odds ratio in favor of the US winning more golds than silvers, which may be overly optimistic given the uncertainty involved. Consider revising your probability to reflect a more nuanced assessment of the situation, perhaps in the range of 45-55.\n",
      "27021 55 I acknowledge the feedback regarding the inherent unpredictability and the current standing of the US behind China in gold medals. While the US does have strong contenders in the final events, the probability of surpassing their silver medal count is somewhat uncertain. Adjusting the probability slightly lower to reflect these uncertainties and the potential for upsets is reasonable. Thus, a revised probability of 55% better accounts for the inherent unpredictability and the current competitive landscape.\n",
      "Pass 1 of 4 on 27021\n",
      "27021 I concur\n",
      "concur 2\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "fcst.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26957 Will Iran carry out a deadly attack within Israel before September 1, 2024?\n",
      "Forecast 75\n",
      "Rationale I acknowledge the feedback regarding the complexity and potential for diplomatic interventions or changes in internal Iranian decision-making affecting the likelihood of an attack. Adjusting the probability to account for these variables provides a more nuanced assessment. While recent intelligence and military movements suggest a high likelihood of a retaliatory strike, it is prudent to temper confidence considering the dynamic nature of geopolitical events. A probability of 75% reflects a high likelihood while accommodating the inherent uncertainties. \n",
      "\n",
      "27019 Will there be a debate between Kamala Harris and Donald Trump on September 4, 2024?\n",
      "Forecast 50\n",
      "Rationale I appreciate the feedback. The lack of a confirmed agreement from both parties and the strong stance of the Harris campaign indeed weigh heavily on the probability. Considering these factors alongside the historical context of debate negotiations, I revise my probability to 50%, reflecting the equal uncertainty of this event occurring as announced by Trump. \n",
      "\n",
      "27020 Before October 1, 2024, will Anthropic announce on the news section of its website that it is planning an IPO?\n",
      "Forecast 20\n",
      "Rationale Given the current environment and the absence of concrete indicators suggesting an imminent IPO, it is unlikely that Anthropic will announce an IPO before October 1, 2024. The consistent funding streams from significant investors like Google and Amazon provide the company with ample resources, potentially delaying any urgency for going public. Additionally, the economic news surrounding major tech companies and the expressed caution in the market do not favor an IPO announcement in such a short timeframe. \n",
      "\n",
      "27021 Will the USA win more Gold than Silver medals at the Paris 2024 Olympics?\n",
      "Forecast 55\n",
      "Rationale I acknowledge the feedback regarding the inherent unpredictability and the current standing of the US behind China in gold medals. While the US does have strong contenders in the final events, the probability of surpassing their silver medal count is somewhat uncertain. Adjusting the probability slightly lower to reflect these uncertainties and the potential for upsets is reasonable. Thus, a revised probability of 55% better accounts for the inherent unpredictability and the current competitive landscape. \n",
      "\n",
      "27022 Will astronauts Suni Williams and Butch Wilmore be on Earth on September 15, 2024?\n",
      "Forecast 70\n",
      "Rationale Given the latest information, it is clear that there are significant technical issues with the Boeing Starliner capsule that have delayed the astronauts' return. There is a strong possibility that NASA may opt to use a SpaceX Dragon capsule for the rescue, as the Starliner's battery life only extends until early September. While NASA and Boeing express confidence in the Starliner, the unresolved thruster issues and helium leaks make the situation uncertain. Given the contingency planning and urgency portrayed, there is a 70% chance that the astronauts will be back on Earth by September 15, 2024. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fcst.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction posted for  26957\n",
      "Comment posted for  26957\n",
      "Prediction posted for  27019\n",
      "Comment posted for  27019\n",
      "Prediction posted for  27020\n",
      "Comment posted for  27020\n",
      "Prediction posted for  27021\n",
      "Comment posted for  27021\n",
      "Prediction posted for  27022\n",
      "Comment posted for  27022\n"
     ]
    }
   ],
   "source": [
    "fcst.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
